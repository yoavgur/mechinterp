{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from mechinterp.Interpreter import Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "interp = Interpreter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(\"hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Logit Lens Output:\n",
       "\t- Topk tokens: ['hello', 'hey', 'buquerque', ' ›', ' Mara', ' /', '!', ' Expand', ' hello', ' Kitty']\n",
       "\n",
       "\t- Bottomk tokens: ['yrus', 'akespe', ' advoc', 'arians', 'rift', ' helicop', 'ilingual', 'azeera', 'aucas', ' indo']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interpreting hidden activations\n",
    "interp.logit_lens(cache[\"blocks.3.hook_resid_post\"][0,1], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit Lens Output:\n",
      "\t- Topk tokens: [' Anyone', ' Whoever', ' Instead', ' There', ' What', ' Regardless', ' Somebody', ' See', ' Personally', 'Anyone', ' Neither', ' Unless', ' This', ' Everyone', ' Those', ' It', ' Someone', ' Anything', ' Again', ' If']\n",
      "\n",
      "\t- Bottomk tokens: [' parks', ' grazing', ' herds', ' fishes', ' reflex', ' affili', ' diminishing', ' entitle', ' habit', ' gamb', ' jung', '���', ' scars', ' graft', 'itter', ' interfaces', ' enforced', ' accumulated', ' seeming', ' asleep']\n"
     ]
    }
   ],
   "source": [
    "# Interpreting weights\n",
    "print(interp.logit_lens(model.blocks[6].mlp.W_out[1256], use_final_ln=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Layer 8\n",
      "Shape: (3072, 5)\n",
      "First ten rows: [['hou', 'ngth', 'heon', 'lords', 'aco'], ['ions', ' Wonders', 'Agg', ' CrossRef', 'MY'], ['ties', 'ogl', 'org', 'illard', 'allo'], ['isse', 'ries', 'que', 'oshi', ' Witcher'], [' backs', 'cko', 'ornia', 'headed', ' quit'], [' by', 'By', 'by', ' Hag', ' Chao'], [' orient', ' footing', 'ties', ' decoration', ' alarm'], [' graduate', ' undergrad', ' diploma', ' college', 'College'], [' cut', 'omatic', 'Notable', ' axis', ' facto'], [' Pharma', 'encers', ' Xuan', ' advanced', ' Gen']]\n",
      "\n",
      "Attention Layer 9 W_O\n",
      "Shape: (12, 64, 5)\n",
      "First ten rows of first head: [['dial', ' conductor', ' tunes', ' tune', 'ymph'], [' telev', ' Pilot', ' OLED', ' Schmidt', ' occupant'], [' Flavor', ' tasted', ' bottle', ' drank', ' cream'], ['answer', ' operator', ' affiliates', 'bands', 'ramer'], [' Journals', 'Paper', ' parchment', ' Tape', 'NPR'], [' Urs', ' Mast', 'Posts', ' Twitch', ' Tumblr'], [' sold', ' manufact', ' flowering', ' defective', 'acia'], [' listens', ' listened', ' listeners', ' listen', ' volumes'], ['heimer', ' Antioch', ' GC', ' mun', ' Interstellar'], [' soundtrack', ' Cinnamon', ' dir', ' narrator', ' note']]\n"
     ]
    }
   ],
   "source": [
    "# Interpreting entire layers\n",
    "ll_mlp8 = interp.logit_lens(model.blocks[8].mlp.W_out, k=5, use_final_ln=True)\n",
    "ll_attn9 = interp.logit_lens(model.blocks[9].attn.W_O, k=5)\n",
    "\n",
    "print(\"MLP Layer 8\")\n",
    "print(f\"Shape: {ll_mlp8.shape}\")\n",
    "print(f\"First ten rows: {ll_mlp8.top[:10]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Attention Layer 9 W_O\")\n",
    "print(f\"Shape: {ll_attn9.shape}\")\n",
    "print(f\"First ten rows of first head: {ll_attn9.top[0][:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
