{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morg/students/yoavgurarieh/miniconda3/envs/fresh/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "model = HookedTransformer.from_pretrained(\"google/gemma-2-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Callable, Union\n",
    "from jaxtyping import Float\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "class PatchscopesOutput:\n",
    "    def __init__(self, explanation: str | list[str]):\n",
    "        self.explanation = explanation\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Patchscopes Explanation:\\n{self.explanation}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSMappingFunction = Callable[[torch.Tensor], torch.Tensor]\n",
    "InterpTensorType = Union[Float[torch.Tensor, \"... d_model\"], Float[torch.Tensor, \"d_model ...\"]]\n",
    "\n",
    "\n",
    "def patchscopes(\n",
    "        vector: InterpTensorType,\n",
    "        prompt: str,\n",
    "        n: int = 20,\n",
    "        target_token: str | None = None,\n",
    "        target_position: int | None = None,\n",
    "        mapping_function: PSMappingFunction = lambda x: x,\n",
    "        target_model: HookedTransformer | None = None,\n",
    "        target_layer: int = 1,\n",
    "        temperature: float = 0.5,\n",
    "    ) -> PatchscopesOutput:\n",
    "    target_model = target_model if target_model is not None else model\n",
    "\n",
    "    # TODO: handle multi dim case\n",
    "    assert vector.shape == (target_model.cfg.d_model,), f\"Vector must be of shape (d_model={target_model.cfg.d_model},), got {vector.shape}\"\n",
    "    assert (target_token is None) != (target_position is None), \"Exactly one of target_token and target_position must be set\"\n",
    "\n",
    "    prompt_toks = target_model.to_tokens(prompt)\n",
    "\n",
    "    if target_position is None:\n",
    "        target_position = target_model.get_token_position(target_token, prompt_toks)\n",
    "\n",
    "    hook_ran = False\n",
    "    def hook_patch_in_act(tensor: torch.Tensor, hook: HookPoint) -> torch.Tensor | None:\n",
    "        nonlocal hook_ran\n",
    "        if not hook_ran:\n",
    "            tensor[:, target_position] = vector\n",
    "            hook_ran = True\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    with target_model.hooks(fwd_hooks=[(f\"blocks.{target_layer}.hook_resid_pre\", hook_patch_in_act)]):\n",
    "        generated_toks = target_model.generate(prompt_toks, max_new_tokens=n, verbose=False, temperature=temperature, use_past_kv_cache=True)\n",
    "\n",
    "    return PatchscopesOutput(target_model.to_string(generated_toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mechinterp import Interpreter\n",
    "interp = Interpreter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit Lens Output:\n",
      "\t- Topk tokens: [' their', 'Their', ' deres', ' Their', ' theirs', ' themselves', 'their', 'sy', ' deras', ' loro', '他们的', '他們的', 'irs', 'ÍT', ' THEIR', ' website', ' leur', ' Haupts', ' hope', 'themselves']\n",
      "\n",
      "\t- Bottomk tokens: ['parsedMessage', ' AttributeSet', 'kuuta', 'PerformLayout', 'GEBURTSDATUM', 'WebElementEntity', ' StringTokenizer', 'Rüyada', ' AssemblyCulture', 'invokeLater', 'aarrggbb', 'CppMethod', '\\ue315', 'AttributeSet', 'MLLoader', 'onAttach', ' estekak', 'WebServlet', '+#+', ' ProtoMessage']\n"
     ]
    }
   ],
   "source": [
    "# v = model.embed(model.to_tokens(\" News\", prepend_bos=False)[0,0])\n",
    "v = model.blocks[15].mlp.W_out[58]\n",
    "\n",
    "print(interp.logit_lens(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>',\n",
       " 'Amazon',\n",
       " \"'\",\n",
       " 's',\n",
       " ' former',\n",
       " ' CEO',\n",
       " ' attended',\n",
       " ' the',\n",
       " ' Oscars']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(\"Amazon's former CEO attended the Oscars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Logit Lens Output:\n",
       "\t- Topk tokens: [' and', ',', ' Amazon', 'Amazon', ' CEO', ' amazon', ' has', ' السابق', ' fondateur', ' chief', ' turned', ' in', ' famously', ' company', ' founder', ' who', ' emeritus', ' says', ' now', ' business']\n",
       "\n",
       "\t- Bottomk tokens: ['########.', ' AssemblyCulture', 'findpost', '<bos>', ' @\"/', 'styleType', '+:+', ' typelib', \")':\", 'uxxxx', ' CreateTagHelper', 'Datuak', 'MigrationBuilder', ' Reverso', 'ANSA', ' للمعارف', 'UnsafeEnabled', ' ModelExpression', 'esgue', 'ỡng']"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model.run_with_cache(\"Amazon's former CEO attended the Oscars\")[1][\"blocks.19.hook_resid_pre\"][0, 5]\n",
    "interp.logit_lens(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Logit Lens Output:\n",
       "\t- Topk tokens: [' Apple', 'Apple', ' apple', ' and', 'apple', ',', ' APPLE', ' CEO', ' turned', ' company', 'apples', ' emeritus', ' démission', ' Cupertino', ' publicly', ' ousted', ' Apfel', ' famously', ' السابق', 'APPLE']\n",
       "\n",
       "\t- Bottomk tokens: ['########.', ' AssemblyCulture', 'findpost', '<bos>', ' }}\"></', 'MigrationBuilder', 'TagMode', 'SBATCH', 'esgue', 'styleType', ' chande', 'ütfen', ' Penh', ' disambiguazione', ']\").', '+:+', 'UrlResolution', ' Winaray', ' Picchu', \")':\"]"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = model.run_with_cache(\"Apple's former CEO attended the Oscars\")[1][\"blocks.19.hook_resid_pre\"][0, 5]\n",
    "interp.logit_lens(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>cat->cat; 135->135; hello->hello; X-> Jeff Bezos is the richest person in the world, with a net worth of $121 billion. 135->135; hello->hello; 135->135; hello->hello; 13\n"
     ]
    }
   ],
   "source": [
    "# ps = patchscopes(v, \"The birth name of X is\", target_token=\" X\", temperature=0.3, target_layer=2, n=50)\n",
    "ps = patchscopes(v, \"cat->cat; 135->135; hello->hello; X->\", target_position=-1, temperature=0.3, target_layer=2, n=50)\n",
    "\n",
    "print(ps.explanation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = model.hook_dict[\"blocks.2.hook_resid_pre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hp.fwd_hooks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LensHandle(hook=<torch.utils.hooks.RemovableHandle object at 0x758763f21fd0>, is_permanent=False, context_level=None)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2, 1576]], device='cuda:0')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens(\" X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_position(prompt: Float[torch.Tensor, \"pos\"] | Float[torch.Tensor, \"1 pos\"], token: int | str) -> int:\n",
    "    if isinstance(token, str):\n",
    "        token = model.to_single_token(token) # type: ignore\n",
    "\n",
    "    nz = (prompt == token).nonzero()\n",
    "\n",
    "    if nz.shape[0] == 0:\n",
    "        raise ValueError(f\"Token '{model.to_string(token)}' not found in prompt\")\n",
    "\n",
    "    if nz.shape[0] > 1:\n",
    "        raise ValueError(f\"Token '{model.to_string(token)}' found multiple times in prompt: {nz.tolist()}\")\n",
    "\n",
    "    return model.get_token_position(token, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'X', ' ', 'X', ' {}'] [1, 3]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def count_placeholders(s):\n",
    "    formatter = string.Formatter()\n",
    "    return sum(1 for _, field_name, _, _ in formatter.parse(s) if field_name is not None)\n",
    "\n",
    "def split_by_placeholders(s):\n",
    "    formatter = string.Formatter()\n",
    "\n",
    "    output = []\n",
    "    output_str = \"\"\n",
    "\n",
    "    for prefix, field_name, _, _ in formatter.parse(s):\n",
    "        output_str += prefix\n",
    "\n",
    "        if field_name is None:\n",
    "            continue\n",
    "\n",
    "        output.append(output_str)\n",
    "        output_str = \"\"\n",
    "\n",
    "    output.append(output_str)\n",
    "\n",
    "    return output\n",
    "\n",
    "def join_list(tok, lst, model: HookedTransformer, prepend_bos: bool = True) -> tuple[list[int], list[int]]:\n",
    "    output = []\n",
    "    indices = []\n",
    "\n",
    "    if not lst:\n",
    "        return output, indices\n",
    "    \n",
    "    bos = model.to_tokens(\"\", prepend_bos=prepend_bos)[0,0].item()\n",
    "\n",
    "    output.append(bos)\n",
    "    output.extend(model.to_tokens(lst[0], prepend_bos=False).tolist()[0])\n",
    "\n",
    "    for item in lst[1:]:\n",
    "        indices.append(len(output))\n",
    "        output.append(tok)\n",
    "\n",
    "        output.extend(model.to_tokens(item, prepend_bos=False).tolist()[0])\n",
    "\n",
    "    return output, indices\n",
    "\n",
    "def format_toks(model: HookedTransformer, prompt, tok=\"X\", prepend_bos: bool = True) -> tuple[torch.Tensor, list[int]]:\n",
    "    tok = model.to_single_token(tok)\n",
    "    splits = split_by_placeholders(prompt)\n",
    "    output, indices = join_list(tok, splits, model, prepend_bos)\n",
    "    return torch.tensor(output), indices\n",
    "\n",
    "# Examples:\n",
    "s1 = \"Hello {} and {} literal {{}}\"\n",
    "s2 = \"He{}llo and literal {{}}\"\n",
    "s3 = \"No placeholders here: {{}}\"\n",
    "s4 = \"{} {} {{}}\"\n",
    "\n",
    "prompt, indices = format_toks(model, s4)\n",
    "print(model.to_str_tokens(prompt), indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTITY_FEW_SHOT = \"cat->cat; 135->135; hello->hello; {}->\"\n",
    "DESCRIPTION_FEW_SHOT = \"Syria: Country in the Middle East, Leonardo DiCaprio: American actor, Samsung: South Korean multinational major appliance and consumer electronics corporation, {}:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchscopes(\n",
    "        vector,\n",
    "        prompt: str,\n",
    "        n: int = 30,\n",
    "        target_model: HookedTransformer | None = None,\n",
    "        target_layer: int = 1,\n",
    "        temperature: float = 0.5,\n",
    "    ) -> PatchscopesOutput:\n",
    "    target_model = target_model if target_model is not None else model\n",
    "\n",
    "    if len(vector.shape) == 1:\n",
    "        vector = vector.unsqueeze(0)\n",
    "\n",
    "    # TODO: handle multi dim case\n",
    "    assert len(vector.shape) <= 2, f\"Vector must be (d_model,) or (batch_size, d_model), got {vector.shape}\"\n",
    "\n",
    "    num_placeholders = count_placeholders(prompt)\n",
    "\n",
    "    assert num_placeholders == vector.shape[0], f\"Prompt must contain {vector.shape[0]} placeholders, got {num_placeholders}.\"\n",
    "\n",
    "    prompt_toks, indices = format_toks(target_model, prompt)\n",
    "\n",
    "    hook_ran = False\n",
    "    def hook_patch_in_act(tensor: torch.Tensor, hook: HookPoint) -> torch.Tensor | None:\n",
    "        nonlocal hook_ran\n",
    "        if not hook_ran:\n",
    "            for i in range(vector.shape[0]):\n",
    "                tensor[:, indices[i]] = vector[i]\n",
    "            hook_ran = True\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    with target_model.hooks(fwd_hooks=[(f\"blocks.{target_layer}.hook_resid_pre\", hook_patch_in_act)]):\n",
    "        generated_toks = target_model.generate(prompt_toks.unsqueeze(0), max_new_tokens=n, verbose=False, temperature=temperature, use_past_kv_cache=True)\n",
    "\n",
    "    return PatchscopesOutput(target_model.to_string(generated_toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patchscopes Explanation:\n",
       "['<bos>The companies founded by X are called Apple and Google, but they are not the only ones. There are other companies founded by people who are not the CEO but who have a major impact']"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchscopes(1.5*v1 + v2, \"The companies founded by {} are called\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patchscopes Explanation:\n",
       "['<bos>Syria: Country in the Middle East, Leonardo DiCaprio: American actor, Samsung: South Korean multinational major appliance and consumer electronics corporation, X: American business magnate, philanthropist and media proprietor, 100-meter sprint: Track and field event, 100-meter dash']"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchscopes(v1, DESCRIPTION_FEW_SHOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2, device='cuda:0')"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'llo and literal {}']"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_by_placeholderss(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{}'"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 8), (30, 36)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_placeholder_indices(s):\n",
    "    # Matches a '{' that is not escaped, then lazily captures until the first unescaped '}'\n",
    "    pattern = r'(?<!{){(?!{).*?}(?!})'\n",
    "    return [(match.start(), match.end()) for match in re.finditer(pattern, s)]\n",
    "\n",
    "# Example usage:\n",
    "s = \"Hello {} and literal {{}} and {name}\"\n",
    "print(get_placeholder_indices(s))\n",
    "# Output: [(6, 8), (28, 34)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat->cat; 135->135; hello->hello; X {->'"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cat->cat; 135->135; hello->hello; {} {{->\".format(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat->cat; 135->135; hello->hello; X X X'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cat->cat; 135->135; hello->hello; {} {} {}\".format(*[\"X\"]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat->cat; 135->135; hello->hello; '"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cat->cat; 135->135; hello->hello; {}\".format(\" X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = model.to_tokens(\"The birth name of X is X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_position(model.to_tokens(\"The birth name of X\"), \" X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "tensor([[   2,  651, 5951, 1503,  576, 1576,  603, 1576]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.get_token_position(\" X\", model.to_tokens(\"The birth name of X is X\")))\n",
    "print(model.to_tokens(\"The birth name of X is X\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
