{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from mechinterp.Interpreter import Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e35cce528f043d5b25ab3f685b05da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morg/students/maorlavi/miniconda3/envs/probing_fakepedia/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-12b-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-12b-deduped\")\n",
    "interp = Interpreter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(\"It was the best of times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuned Logit Lens Output:\n",
       "\t- Topk tokens: [' times', ' all', '\\n', ' and']\n",
       "\n",
       "\t- Bottomk tokens: ['theless', 'medsc', '        ', '     ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp.tuned_lens(cache[\"blocks.15.hook_resid_post\"][0, 5], l=15, k=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef7b9c160b34f979a80cf131a3fef7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' possible', ' thing', '-', ' way', ' part', ' place', ' I', ' kind', ' we', ' time']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tuned_lens import TunedLens\n",
    "\n",
    "model_name = \"EleutherAI/pythia-12b-deduped\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "lens = TunedLens.from_model(model)\n",
    "input_text = \"It was the best of times\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states  \n",
    "l = 15\n",
    "k = 10\n",
    "hidden_state_l = hidden_states[l]\n",
    "logits = lens(h = hidden_state_l, idx = l)\n",
    "\n",
    "top_k_probs, top_k_indices = torch.topk(logits[0, 3, :], k)\n",
    "\n",
    "top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_indices]\n",
    "\n",
    "print(top_k_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tuned_lens import load_artifacts\n",
    "\n",
    "artifacts = load_artifacts.load_lens_artifacts(\"EleutherAI/pythia-12b-deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/morg/students/maorlavi/models/huggingface/hub/spaces--AlignmentResearch--tuned-lens/snapshots/1ac7285852a22309f571c2555efc37375d0c4cda/lens/EleutherAI/pythia-12b-deduped/params.pt')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifacts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' times', ' all', '\\n', ' life', ' the', ' time', 's', ' T', ' and', '-']\n"
     ]
    }
   ],
   "source": [
    "l = 23\n",
    "k = 10\n",
    "hidden_state_l = hidden_states[l+2]\n",
    "logits = lens(h = hidden_state_l, idx = l)\n",
    "\n",
    "top_k_probs, top_k_indices = torch.topk(logits[0, 4, :], k)\n",
    "\n",
    "top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_indices]\n",
    "\n",
    "print(top_k_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(hidden_states))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
